---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
# Load the package required
library("jsonlite")
library(tibble)
library(yelpr)
library(dplyr)

# Get my API key
key = readLines("Yelp API.txt", warn = F)


#I tried a few times and it seems borough by borough is best 
#Manhattan restaurant calls 
resturants = data.frame()
for (i in 0:99) {
  
  temp = business_search(api_key = key,
                  location = 'Manhattan',
                  term = "resturants",
                  limit = 50,
                  offset = 50*i)
  resturants = bind_rows(resturants, as.data.frame(temp))
}
#Brooklyn restaurant calls 
for (i in 0:99) {
  
  temp = business_search(api_key = key,
                  location = 'Brooklyn',
                  term = "resturants",
                  limit = 50,
                  offset = 50*i)
  resturants = bind_rows(resturants, as.data.frame(temp))
}
#Queens restaurant calls 
for (i in 0:99) {
  
  temp = business_search(api_key = key,
                  location = 'Queens',
                  term = "resturants",
                  limit = 50,
                  offset = 50*i)
  resturants = bind_rows(resturants, as.data.frame(temp))
}
#Bronx restaurant calls
for (i in 0:99) {
  
  temp = business_search(api_key = key,
                  location = 'Bronx',
                  term = "resturants",
                  limit = 50,
                  offset = 50*i)
  resturants = bind_rows(resturants, as.data.frame(temp))
}

dim(resturants)
unique(resturants$businesses.location$city)
head(resturants)

cor(resturants$businesses.rating, resturants$businesses.review_count)
```

```{r}
businesses_test = data.frame()
index = 1
# Add in offset to do 5000 daily pulls. Then save to csv file 
for (biz in covid_yelp_tbl$business_id[1:100]){
  temp = business_lookup_id(api_key = key, 
                            business_id = biz)
  
  #this for loop section goes into bad data entries and removes them
  #too many different options depending on what the category was to make something nicer
  for (i in names(temp)) {
  t = temp[i][[1]]
  if (length(t) > 1 & length(names(t)) > 1) {

    for (j in names(t)) {
      #print(j)
      if (is.vector(t[j]) == T & length(t[j][[1]]) > 1) {
        #print(t[j])
        temp[i][[1]][j][[1]] = NA
      }
    }
  } else if (length(t) > 1) {
      temp = temp[-which(names(temp) %in% c(i))]
    }
  }
  temp_biz = bind_cols(as.data.frame(bind_cols(temp)[1,]), covid_yelp_tbl[index,])

  businesses_test = bind_rows(businesses_test, temp_biz)
  index = index + 1
  if (index %% 100 == 0) {
    print(index)
  }
}

head(businesses_test)
```


```{r}
table(resturants$businesses.is_closed, useNA = "ifany")
```
LG: This is a problem - can we accurately conclude that NA fields are restaurants that have closed? 

```{r}
hist(resturants$businesses.rating,xlab = "ratings out of 5", main = "Histogram of business ratings")
```
```{r}
table(resturants$businesses.price, useNA = "ifany")
```

```{r}
barplot(height = table(resturants$businesses.price,  useNA = "ifany"), 
        names.arg = names(table(resturants$businesses.price, useNA = "ifany")))
```
```{r}
hist(resturants$businesses.review_count, xlab = "Number of reviews", main = "Histogram of review Dist")
```

Methods just to get mine imported too
```{r}
#yelp_biz = jsonlite::stream_in(file( "/Users/zacmacintyre/Desktop/Git Folder/yelp_dataset/yelp_academic_dataset_business.json"))
#yelp_flat = jsonlite::flatten(yelp_biz)
#yelp_tbl = as_tibble(yelp_flat)
# However, most of this data is from BC, CO, FL, GA, MA, OH, OR, and TX (Not NY)
#table(yelp_tbl$state)
# It is a very large dataset
#dim(yelp_tbl)
#names(yelp_tbl)
#Include more than restaurants 
#yelp_tbl %>% mutate(categories = as.character(categories)) %>% select(categories)

#COVID_data = jsonlite::stream_in(file("/Users/zacmacintyre/Desktop/Git Folder/covid_19_dataset_2020_06_10/yelp_academic_dataset_covid_features.json"))
#covid_flat = jsonlite::flatten(COVID_data)
#covid_yelp_tbl = as_tibble(covid_flat)
```

Yelp Dataset for Academic purposed as of March 2020


```{r}
yelp_biz = jsonlite::stream_in(file( "/Users/lukasgeiger/Desktop/Columbia/SeniorYear/Spring2021/AppliedDataMining/FinalProject/yelp_dataset/yelp_academic_dataset_business.json"))
yelp_flat = jsonlite::flatten(yelp_biz)
yelp_tbl = as_tibble(yelp_flat)
# However, most of this data is from BC, CO, FL, GA, MA, OH, OR, and TX (Not NY)
table(yelp_tbl$state)
# It is a very large dataset
dim(yelp_tbl)
names(yelp_tbl)
#Include more than restaurants 
yelp_tbl %>% mutate(categories = as.character(categories)) %>% select(categories)
```

```{r}
COVID_data = jsonlite::stream_in(file( "/Users/lukasgeiger/Desktop/Columbia/SeniorYear/Spring2021/AppliedDataMining/FinalProject/covid_19_dataset_2020_06_10/yelp_academic_dataset_covid_features.json"))
covid_flat = jsonlite::flatten(COVID_data)
covid_yelp_tbl = as_tibble(covid_flat)
```

This code doesnt actually work.  As you pull in data it is unfortunately in the wrong format a bunch.  I have corrected code down below.  It is still a 
```{r}
combined_tbl = yelp_tbl %>% right_join(covid_yelp_tbl)
covid_yelp_tbl = as.data.frame(covid_yelp_tbl)
# There are only two businesses with the same business id in the two tibbles.
```


```{r}
businesses = data.frame()
index = 1
# Add in offset to do 5000 daily pulls. Then save to csv file 
for (biz in covid_yelp_tbl$business_id[1:5000]){
  temp = business_lookup_id(api_key = key, 
                            business_id = biz)
  temp_biz = bind_cols(as.data.frame(t(cbind(temp))), covid_yelp_tbl[index,])
#for (biz in covid_yelp_tbl$business_id[1:10]){
#  temp = business_lookup_id(api_key = key, 
#                            business_id = biz)
    
#  temp = temp[-which(names(temp) %in% c("photos", "photo", 'display_address'))]
#  temp_biz = bind_cols(as.data.frame(bind_cols(temp)[1,]), covid_yelp_tbl[index,])
  #temp_biz = temp_biz[,-which(names(temp_biz) %in% c("photos"))]


  businesses = bind_rows(businesses, temp_biz)
  #print(businesses)
  index = index + 1
  #if (index %% 100 == 0) {
  #  print(index)
  #}
}
head(businesses)
```

```{r}
# this assumes that all columns will have the same length. Rather there will be some columns that have NA fields. Need to write a function that goes line by line through each row in the data frame and unlist and combines them otherwise write NA. TODO
set_lists_to_chars <- function(x) {
    if(class(x) == 'list') {
    y <- paste(unlist(x[1]), sep='', collapse=', ')
    } else {
    y <- x 
    }
    return(y)
}

businesses = saved_biz
nms = names(businesses)
col = c()
for (row in businesses[,1]) {
  col = c(col,set_lists_to_chars(row))
}
bizzes = matrix(ncol=length(nms), nrow=length(col))
colnames(bizzes) = nms
bizzes[,1] = col
for (row in 1:nrow(businesses)){
  for (col in 2:length(businesses)) {
    bizzes[row,col] = set_lists_to_chars(businesses[row,col])
  }
}

bizzes = as.data.frame(bizzes)
head(bizzes)

write.table(bizzes, "covid_yelp_biz.csv",append = TRUE,sep = ",") 
```

```{r}
biz = read.csv("covid_yelp_biz.csv")


head(biz)

biz = biz %>% mutate(price =  na_if(price, ""),
                     photos =  na_if(photos, ""),
                     business_id =  na_if(business_id, ""),
                     highlights =  na_if(highlights, ""),
                     transactions = na_if(transactions, ""),
                     coordinates = na_if(coordinates, ""),
                     location = na_if(location, ""),
                     categories = na_if(categories, ""),
                     display_phone = na_if(display_phone, ""),
                     url = na_if(url, ""),
                     image_url = na_if(image_url, ""),
                     name = na_if(name, ""),
                     alias = na_if(alias, ""),
                     id = na_if(id, ""),
                     hours = na_if(hours, ""),
                     business_id = na_if(business_id, ""),
                     Covid.Banner = na_if(Covid.Banner, ""),
                     Temporary.Closed.Until = na_if(Temporary.Closed.Until, ""),
                     Virtual.Services.Offered = na_if(Virtual.Services.Offered, ""),
                     messaging = na_if(messaging, ""),
                     special_hours = na_if(special_hours, ""))

biz$Virtual.Services.Offered = as.logical(gsub('offers_virtual_consultations|offers_virtual_tours|offers_virtual_classes', 'TRUE', biz$Virtual.Services.Offered))
```

```{r}
#getting covid cases data 
#it was in a weird format with lots of blank spaces 
case = read.csv("covid cases.csv")

covid_cases = data.frame()
for (i in 1:51) {
  row = case[i*4 + 1,1:2]
  covid_cases = bind_rows(covid_cases, row)
}
 
covid_cases$Cases = as.numeric(gsub('\\,', '', covid_cases$Cases)) 
#head(covid_cases)


# reading in vaccine data 
vax = read.csv("vaccines.csv")
vaxes = data.frame()
for (i in 1:51) {
  
  c1 = vax[i*2 -1,1]
  c2 = vax[i*2,2:4]
  row = bind_cols(c1,c2)
  vaxes = bind_rows(vaxes, row)
}

names(vaxes) = c("Location", "Doses_given", "Fully_vaccinated", "Population_fully_vaccinated")

vaxes = vaxes %>% mutate(Doses_given = as.numeric(gsub('\\,', '', Doses_given)),
                         Fully_vaccinated = as.numeric(gsub('\\,', '', Fully_vaccinated)),
                         Population_fully_vaccinated = as.numeric(gsub('\\%', '',
                                                                       Population_fully_vaccinated)))

#merging into 1 DF
covid = merge(vaxes, covid_cases, by = "Location")
head(covid)
```
```{r}
#feature creation section 
covid$Population_fully_vaccinated = covid$Population_fully_vaccinated / 100
covid$feature = covid$Cases * (1 - covid$Population_fully_vaccinated) 

#scaling the feature between 0 and 1 
covid$scaled_feature = (covid$feature - min(covid$feature)) / (max(covid$feature) - min(covid$feature))

```


```{r}
# Feature engineering - highly correlated things turn into one variable, maybe combine # of dollar signs with business rating, to reduce dimensions. Common on interviews. Ratings are variable to the number of rating recieved. 10 ratings will swing a lot while 1000 ratings will swing less with each additional rating. 
# Another option is to get simple sentiment analysis of reviews distilled to a number. 
# Could also add a density mapping based on the city the restaurant is in. Or a score of how strict the city is iwth COVID. 

```

```{r}
table(biz$is_closed , useNA = "ifany")
```
This is actually pretty good for us so far, we dont have crazy disproportionate classes 

```{r}
hist(biz$review_count[biz$review_count < 100], breaks = 20, main = "Hist of reviews that have less than 100")

hist(biz$review_count[biz$review_count > 100], breaks = 20, main = "Hist of reviews that have over 100")

barplot(height = table(biz$rating,  useNA = "ifany"), 
        names.arg = names(table(biz$rating, useNA = "ifany")), xlab = "rating of the biz")

barplot(height = table(biz$price,  useNA = "ifany"), 
        names.arg = names(table(biz$price, useNA = "ifany")), xlab = "prices of the biz")
```
```{r}
# all the NAs in each col
colSums(is.na(biz))

#total missing values 
percent_missing = sum(colSums(is.na(biz))) / (3600 *30)
percent_missing

```



```{r}
#wrote this so you didnt have to change the actual DF
#probably makes sense to do so and did it after
prices = rep(NA, length(biz$price))
length(prices)

for (i in 1:length(biz$price)) {
  if (!is.na(biz$price[i]) & biz$price[i] == '$') {
    prices[i] = 1
  } else if (!is.na(biz$price[i]) & biz$price[i] == '$$') {
      prices[i] = 2
  } else if (!is.na(biz$price[i]) & biz$price[i] == '$$$') {
      prices[i] = 3
  } else if (!is.na(biz$price[i]) & biz$price[i] == '$$$$') {
      prices[i] = 4
  }
}

```

```{r}
biz$price = prices 
```

```{r}
#geting some of the cor for things that are numeric
cor(biz$rating, prices, use = "pairwise.complete.obs")
cor(biz$rating, biz$review_count, use = "pairwise.complete.obs")
cor(biz$price, biz$review_count, use = "pairwise.complete.obs")
```

```{r}
#geting some of the cor for things that are numeric
#specifically looking at it with is the business closed
cor(biz$is_closed, biz$price, use = "pairwise.complete.obs")
cor(biz$is_closed, biz$review_count, use = "pairwise.complete.obs")
cor(biz$is_closed, biz$rating, use = "pairwise.complete.obs")
```

```{r}
#running a logistic regression on the data that has numeric or boolean values 
y = biz$is_closed
x1 = biz$review_count
x2 = biz$rating
x3 = biz$price
x4 = biz$delivery.or.takeout
x5 = biz$Grubhub.enabled
x6 = biz$Virtual.Services.Offered

#i am goiing to have to maybe make a new way to test/train split for evaluation
#but it works for the moment
train <- sample(c(TRUE, FALSE), nrow(biz), replace=TRUE)
mod <- glm(y ~ x1+x2+x3+x4+x5+x6, family=binomial(link="logit"),
           subset=train)
test_probs <- predict(mod, newdata=data.frame(x=x[!train]), type="response")

test_pred = test_probs > .5

#still need a way to evaluate the model
```


```{r}

```

















