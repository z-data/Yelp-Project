if (!is.na(year) & year == '2015') {
df311$year[i] = 2015
} else if(!is.na(year) & year == '2016') {
df311$year[i] = 2016
} else if (!is.na(year)){
df311$year[i] = 2017
}
}
knitr::opts_chunk$set(echo = TRUE)
# Load the package required to read JSON files.
library("rjson")
library(dplyr)
votes = fromJSON(file = "votes.json")
#niave way to make a df
df = as.data.frame(votes)
#getting senators names
senators = names(votes)
#initialize a DF
senators_DF = as.data.frame(votes[[1]])
row.names(senators_DF) = senators[1]
#making  DF out of all the senator data
for (i in 2:length(senators)) {
temp = as.data.frame(votes[[i]])
row.names(temp) = senators[i]
senators_DF = bind_rows(senators_DF, temp)
}
#quick transpose so i can easily look up senators
senators_DF = as.data.frame(t(as.matrix(senators_DF)))
dim(senators_DF)
voting_matirx = as.matrix(senators_DF)
#function to detrmine data that is missing
is_bad = function(x) {
if (x == -9999) {
return(T)}
else {
return(F)
}
}
#getting the error in data collection values and NA values
bad = sum(apply(df,2, is_bad))
missing = sum(is.na(senators_DF))
entries = 803 * 231
percent_bad = (missing + bad) / entries
percent_bad
corelation_m = cor(voting_matirx, use='pairwise.complete.obs')
lower = lower.tri(voting_matirx)
hist(corelation_m[lower], xlab = 'correlations of lower matrix')
#getting a list of indexes that correspond to being correlated to mitch
mitch = order(corelation_m["S174",], decreasing = T, na.last = NA)
#making a new matrix where all the rows and columns are ordered by
corelation_m = corelation_m[mitch,mitch]
#reading online it seems image automatically rotates the matrix
#this counteracts that
rotate <- function(x) t(apply(x, 2, rev))
image(rotate(corelation_m))
library(dplyr)
with_mitch = corelation_m["S174",]
#getting a list of republicans that arent mitch
repubs = with_mitch[with_mitch > .2 & with_mitch < 1]
#getting a list of people who have neg correlation with mitch
neg_w_mitch = with_mitch[with_mitch < 0]
#matrix with columns of neg with mitch and rows of probable republicans
new_mat = corelation_m[names(repubs), names(neg_w_mitch)]
#applying the mean function to all columns and grabbing values above .2
sens = apply(new_mat,2,mean, na.rm = T)
repub_not_wmitch = sens[sens > .2]
name = names(repub_not_wmitch)
#importing in the voters file
voters = fromJSON(file = "voters.json")
#getting a list of the actual names of the senators that are republican
#but who are also negatively correlated with Mitch McConnell
names_li = c()
for (i in 1:length(name)) {
first = voters[name[i]][[1]][1]
last = voters[name[i]][[1]][2]
full_name = paste(first,last)
names_li[i] = full_name
}
print(names_li)
#method to get senators by last name
senators_numbs = names(voters)
numb = NA
for (i in 1:length(senators_numbs)) {
last = voters[senators_numbs[i]][[1]][2]
if (last == "McCain") {
numb = senators_numbs[i]
}
}
voters[numb]
library(caret)
set.seed(123)
#getting an order of senators that are either alike or dislike
sen = order(corelation_m[numb,], decreasing = T, na.last = NA)
#corelation_m[sen,sen]
#got the senators using the commented out corelation_m matrix
model = lm(S355~ S313 + S340, data = senators_DF)
summary(model)
#creating a new data matrix with just these 3 senators
new_df_method1 = cbind(senators_DF$S355, senators_DF$S340, senators_DF$S313)
new_df_method1 = na.omit(new_df_method1)
colnames(new_df_method1) = c("S355","S340","S313")
new_df_method1 = as.data.frame(new_df_method1)
#grabbing 20% of samples to test
is_test = sample(1:length(new_df_method1[,1]), .2*length(new_df_method1[,1]))
#my model
model_m1 = lm(S355 ~ S340 + S313, data = new_df_method1[-is_test,1:3])
#predictions
preds = predict(model_m1, newdata = new_df_method1[is_test,1:3])
#this function rounds to the nearest -1,0,1
#makes it able to see how the predictions would have done
#useful because votes are yea, no vote, nay no inbetweens
prediction_rounding = function(preds) {
for(i in 1:length(preds)) {
if (preds[i] < .5 & preds[i] > -.5) {preds[i] = 0}
else if (preds[i] > .5) {preds[i] = 1}
else{preds[i] = -1}
}
return(preds)
}
#getting predicted and true values
preds = prediction_rounding(preds)
truth = new_df_method1[is_test,1]
#making the confusion matrix
confusion = as.data.frame(cbind(preds,truth))
confusionMatrix(factor(confusion$preds), factor(confusion$truth))
#creating a new data matrix with just these 3 senators
new_df_method1 = cbind(senators_DF$S355, senators_DF$S340, senators_DF$S313)
new_df_method1 = na.omit(new_df_method1)
colnames(new_df_method1) = c("S355","S340","S313")
new_df_method1 = as.data.frame(new_df_method1)
#grabbing 20% of samples to test
is_test = sample(1:length(new_df_method1[,1]), .2*length(new_df_method1[,1]))
#my model
model_m1 = lm(S355 ~ S340 + S313, data = new_df_method1[-is_test,1:3])
#predictions
preds = predict(model_m1, newdata = new_df_method1[is_test,1:3])
#this function rounds to the nearest -1,0,1
#makes it able to see how the predictions would have done
#useful because votes are yea, no vote, nay no inbetweens
prediction_rounding = function(preds) {
for(i in 1:length(preds)) {
if (preds[i] < .5 & preds[i] > -.5) {preds[i] = 0}
else if (preds[i] > .5) {preds[i] = 1}
else{preds[i] = -1}
}
return(preds)
}
#getting predicted and true values
preds = prediction_rounding(preds)
truth = new_df_method1[is_test,1]
#making the confusion matrix
confusion = as.data.frame(cbind(preds,truth))
confusionMatrix(factor(confusion$preds), factor(confusion$truth))
#plotting the truth vs the predicted
x = seq(1, length(preds),1)
plot(y = preds, x , col = "red", ylim = c(-1.5,1.5), ylab = "Values of the Votes")
points(y = truth, x, pch = 20, cex = .5)
legend(11,.8,legend=c("Expected", "Real"),
col=c("red", "black"), pch = c(1,20), cex = 1)
library(caret)
#getting an order of senators that are either alike or dislike
sen = order(corelation_m[numb,], decreasing = T, na.last = NA)
voting_matrix[sen[1]]
voting_matirx[sen[1]]
library(caret)
#getting an order of senators that are either alike or dislike
sen = order(corelation_m[numb,], decreasing = T, na.last = NA)
voting_matirx[sen[1]]
voting_matrix[sen[length[sen]]]
library(caret)
#getting an order of senators that are either alike or dislike
sen = order(corelation_m[numb,], decreasing = T, na.last = NA)
voting_matirx[sen[1]]
voting_matirx[sen[length[sen]]]
library(caret)
#getting an order of senators that are either alike or dislike
sen = order(corelation_m[numb,], decreasing = T, na.last = NA)
voting_matirx[sen[1]]
voting_matirx[sen[length(sen)]]
#got the senators using the commented out corelation_m matrix
model = lm(S355~ S313 + S340, data = senators_DF)
summary(model)
df2 = read.csv("tweet_token_freq.csv")
head(df2)
dim(df2)
target <- 1373
samps <- sample(nrow(df2), 4)
df2[target, "text"]
target_freq <- as.numeric(df2[target, -(1:9)])
freqs_mat <- as.matrix(df2[, -(1:9)])
diffs <- abs(freqs_mat - matrix(rep(target_freq,  nrow(freqs_mat)),
byrow=TRUE, nrow=nrow(freqs_mat)))
dist_to_target_tweet <- apply(diffs, 1, sum)
ord = order(dist_to_target_tweet)
head(ord, 10)
4I/3I
as.integer(6)/as.interger(4)
as.integer(6)/as.integer(4)
sample(df3111, 10000)
sample(df311, 10000)
dim(df311)
dim(df311)
index = sample(nrows(df311), 10000)
nrows(df311)
index = sample(nrow(df311), 10000)
dim(df311)
index = sample(nrow(df311), 10000)
samp = df311[index,]
head(samp)
table(samp$complaint_type)
table(samp$complaint_type)
samp = as.matrix(samp)
cor_samp = cor(samp)
samp = df311[index,]
head(samp)
table(samp$desecriptor)
table(samp$descriptor)
table(samp$agency_name)
as.matrix(table(samp$agency_name))
index = sample(nrow(city_pay), 10000)
samp_c = city_pay[index,]
head(city_pay,10)
index
head(city_pay,10)
head(city_pay,10)
head(samp_c,10)
agency = as.matrix(table(samp$agency_name))
agency[1]
agency[,1]
agency
agency[1,]
agency = as.data.frame(table(samp$agency_name))
agency
agency = as.data.frame(t(table(samp$agency_name)))
agency
agency = as.matrix(t(table(samp$agency_name)))
agency
agency = as.data.frame(agency)
agency
agency = as.matrix(t(table(samp$agency_name)))
agency = as.data.frame(agency)
agency
names(table(samp$agency_name))
length(names(df311$agency_name))
length(unique(df311$agency_name))
df311$year = NA
for (i in 1:length(df311$year)) {
year = substr(df311$closed_date[i],1,4)
if (!is.na(year) & year == '2015') {
df311$year[i] = 2015
} else if(!is.na(year) & year == '2016') {
df311$year[i] = 2016
} else if (!is.na(year)){
df311$year[i] = '2017'
}
}
head(samp)
class(samp$created_date)
samp$created_date[1]
samp$created_date[1] > as.POSIXct(2015-09-20, format = "%Y-%m-%d")
samp$created_date[1] > as.POSIXct("2015-09-20", format = "%Y-%m-%d")
head(samp_c,10)
#getting initial samples
index = sample(nrow(city_pay), 10000)
samp_city_pay = city_pay[index,]
head(samp_city_pay,10)
index = sample(nrow(df311), 10000)
s311 = df311[index,]
head(s311)
#looked up nyc city fiscal year, it starts july 1st
#want to make a function that takes the date and just gives back the year
to_year = function(df) {
df$year = NA
for(i in 1:length(df$year)) {
year = df$created_date
if (!is.na(year) & year > as.POSIXct("2016-06-30", format = "%Y-%m-%d")) {
df311$year[i] = 2017
} else if(!is.na(year) & year < as.POSIXct("2016-06-30", format = "%Y-%m-%d") & year > as.POSIXct("2015-06-30", format = "%Y-%m-%d")) {
df311$year[i] = 2016
} else if (!is.na(year)){
df311$year[i] = 2015
}
}return(df)
to_year = function(df) {
df$year = NA
for(i in 1:length(df$year)) {
year = df$created_date
if (!is.na(year) & year > as.POSIXct("2016-06-30", format = "%Y-%m-%d")) {
df311$year[i] = 2017
} else if(!is.na(year) & year < as.POSIXct("2016-06-30", format = "%Y-%m-%d") & year > as.POSIXct("2015-06-30", format = "%Y-%m-%d")) {
df311$year[i] = 2016
} else if (!is.na(year)){
df311$year[i] = 2015
}
} return(df)
to_year = function(df) {
df$year = NA
for(i in 1:length(df$year)) {
year = df$created_date
if (!is.na(year) & year > as.POSIXct("2016-06-30", format = "%Y-%m-%d")) {
df311$year[i] = 2017
} else if(!is.na(year) & year < as.POSIXct("2016-06-30", format = "%Y-%m-%d") & year > as.POSIXct("2015-06-30", format = "%Y-%m-%d")) {
df311$year[i] = 2016
} else if (!is.na(year)){
df311$year[i] = 2015
}
}
return(df)
}
s311 = to_year(s311)
# Load the package required to read JSON files.
library("rjson")
# Give the input file name to the function.
result <- fromJSON(file = "yelp_academic_dataset_business.json")
# Print the result.
print(result)
# Load the package required to read JSON files.
library("rjson")
# Give the input file name to the function.
result <- fromJSON(file = "yelp_academic_dataset_business.json")
# Print the result.
result
# Load the package required to read JSON files.
library("rjson")
# Give the input file name to the function.
result <- fromJSON(file = "yelp_academic_dataset_business.json")
# Print the result.
#result
json_data_frame <- as.data.frame(result)
print(json_data_frame)
dim(json_data_frame)
forest = read.csv("aug_train.csv")
df = read.csv("aug_train.csv")
df$target <- as.factor(df$target)
install.packages(randomForest)
install.packages("randomForest")
df = read.csv("aug_train.csv")
library(randomForest)
library(dplyr)
df$target <- as.factor(df$target)
head(df)
mod = randomForest(traget~. -enrolle_id, df)
mod = randomForest(target~. -enrolle_id, df)
head(df)
mod = randomForest(target~. -enrollee_id, df)
mod
mod = randomForest(target~. -enrollee_id, df, importance = T)
varImpPlot(mod, type=1)
time1 = sys.time()
time1 = Sys.time()
mod = randomForest(target~. -enrollee_id, df)
time2 = Sys.time()
print(time2-time1)
time1 = Sys.time()
mod = randomForest(target~. -enrollee_id, df, importance = T)
time2 = Sys.time()
print(time2-time1)
varImpPlot(mod, type=1)
varImpPlot(mod, type=2)
?runif()
?rbern()
idx = sample(df$city_development_index, len(df$city_development_index), replace = F)
idx = sample(df$city_development_index, length(df$city_development_index), replace = F)
devtools::install_github("OmaymaS/yelpr")
install.packages("OmaymaS/yelpr")
install.packages("yelpr")
library(yelpr)
install.packages("devtools")
devtools::install_github("OmaymaS/yelpr")
install_github("OmaymaS/yelpr")
install.packages("install_github")
remotes::install_github("ropensci/rnaturalearthhires")
install.packages("remotes")
remotes::install_github("OmaymaS/yelpr")
library("yelpr")
api = read.csv('Yelp API.txt')
api
api = read.delim('Yelp API.txt')
api
api = resume = readLines('Yelp API.txt', warn = F)
api
api = resume = readLines('Yelp API.txt', warn = F)
api
resturant_ny = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 10000)
key = resume = readLines('Yelp API.txt', warn = F)
resturant_ny = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 10000)
resturant_ny
length(resturant_ny)
resturant_ny = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 1000)
length(resturant_ny)
resturant_ny
resturant_ny = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 50)
length(resturant_ny)
resturant_ny
resturant_ny$categories
resturant_ny$businesses$categories
resturant_ny = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 100)
length(resturant_ny)
resturant_ny$businesses$categories
library(dplyr)
resturant_ny = data.frame()
for i in 0:2 {
resturant_ny = data.frame()
for (i in 0:2) {
offset = 50*i
temp = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 50,
offset = offset)
resturant_ny =  bind_rows(resturant_ny, temp)
}
resturant_ny
resturant_ny
resturant_ny = data.frame()
for (i in 0:1) {
offset = 50*i
temp = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 50,
offset = offset)
resturant_ny =  bind_rows(resturant_ny, temp)
}
for (i in 0:1) {
print(i)
}
resturant_ny = data.frame()
for (i in 0:1) {
temp = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 50,
offset = 50*i)
resturant_ny =  bind_rows(resturant_ny, temp)
}
resturant_ny = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 50)
length(resturant_ny)
resturant_ny
class(resturant_ny)
as.data.frame(resturant_ny)
resturant_ny = data.frame()
for (i in 0:1) {
temp = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 50,
offset = 50*i)
resturant_ny =  bind_rows(resturant_ny, as.data.frame(temp))
}
resturant_ny
resturant_ny[49:51,]
resturant_ny = data.frame()
for (i in 0:1000) {
temp = business_search(api_key = key,
location = 'New York',
term = "restaurants",
limit = 50,
offset = 50*i)
resturant_ny =  bind_rows(resturant_ny, as.data.frame(temp))
}
dim(resturant_ny)
resturant_ny$businesses.location$city
unique(resturant_ny$businesses.location$city)
for (i in 0:500) {
temp = business_search(api_key = key,
location = 'Queens',
term = "restaurants",
limit = 50,
offset = 50*i)
resturant_ny =  bind_rows(resturant_ny, as.data.frame(temp))
}
unique(resturant_ny$businesses.location$city)
for (i in 0:500) {
temp = business_search(api_key = key,
location = 'Bronx',
term = "restaurants",
limit = 50,
offset = 50*i)
resturant_ny =  bind_rows(resturant_ny, as.data.frame(temp))
}
dim(resturant_ny)
resturant_ny$businesses.location$zip_code[resturant_ny$businesses.location$zip_code == 10025.]
sum(resturant_ny$businesses.location$zip_code[resturant_ny$businesses.location$zip_code == 10025.])
sum(resturant_ny$businesses.location$zip_code[resturant_ny$businesses.location$zip_code == 10025])
resturant_ny$businesses.location$zip_code[resturant_ny$businesses.location$zip_code == 10025]
resturant_ny$businesses.location$zip_code
sum(resturant_ny$businesses.location$zip_code[resturant_ny$businesses.location$zip_code == "10025"])
resturant_ny$businesses.location$zip_code[resturant_ny$businesses.location$zip_code == "10025",]
resturant_ny[resturant_ny$businesses.location$zip_code == "10025",]
dim(resturant_ny)
getwd()
# Print the result.
key = readline("/Users/zacmacintyre/Desktop/Data Mining/Final Project/Yelp API.txt")
